{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(<PATH>, sep=';', engine='python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create groups for resources and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Grinding = ['Machine 10 - Grinding', 'Machine 11 - Grinding', 'Machine 12 - Grinding', 'Machine 19 - Grinding',\n",
    "            'Machine 25 - Grinding', 'Machine 27 - Grinding']\n",
    "\n",
    "Round_Grinding = ['Machine 2 - Round Grinding', 'Machine 3 - Round Grinding']\n",
    "\n",
    "Turning_and_Milling = ['Machine 4 - Turning & Milling', 'Machine 5 - Turning & Milling', 'Machine 6 - Turning & Milling',\n",
    "                       'Machine 8 - Turning & Milling', 'Machine 9 - Turning & Milling']\n",
    "\n",
    "Turning = ['Machine 15 - Turning', 'Machine 21 - Turning', 'Machine 23 - Turning']\n",
    "\n",
    "Milling = ['Machine 14 - Milling', 'Machine 16 - Milling', 'Machine 22 - Milling']\n",
    "\n",
    "Quality_Check = ['Quality Check 1', 'Quality Check 2']\n",
    "\n",
    "Wire_Cut = ['Machine 18 - Wire Cutting', 'Wire Cut - Machine 13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GrindingRework = ['Grinding Rework','Grinding Rework - Machine 12', 'Grinding Rework - Machine 2', \n",
    "                  'Grinding Rework - Machine 27']\n",
    "\n",
    "Milling = ['Milling - Machine 10','Milling - Machine 14','Milling - Machine 16', 'Milling - Machine 8']\n",
    "\n",
    "FlatGrinding = ['Flat Grinding - Machine 11','Flat Grinding - Machine 26']\n",
    "\n",
    "RoundGrinding = ['Round Grinding - Machine 12', 'Round Grinding - Machine 19', 'Round Grinding - Machine 2',\n",
    "                    'Round Grinding - Machine 23', 'Round Grinding - Machine 3', 'Round Grinding - Manual']\n",
    "\n",
    "TurningMilling = ['Turning & Milling - Machine 10', 'Turning & Milling - Machine 10', \n",
    "                      'Turning & Milling - Machine 4', 'Turning & Milling - Machine 5', \n",
    "                      'Turning & Milling - Machine 6', 'Turning & Milling - Machine 8', \n",
    "                     'Turning & Milling - Machine 9']\n",
    "\n",
    "SetupTurnMill = ['SETUP     Turning & Milling - Machine 5', 'Setup - Machine 8', 'Setup - Machine 4']\n",
    "\n",
    "Turning = ['Turning Machine 21', 'Turning - Machine 4', 'Turning - Machine 5', 'Turning - Machine 8',\n",
    "               'Turning - Machine 9']\n",
    "\n",
    "TurnMillScrew = ['Turn & Mill. & Screw Assem - Machine 9', 'Turn & Mill. & Screw Assem - Machine 10']\n",
    "\n",
    "WireCut = ['Wire Cut - Machine 13', 'Wire Cut - Machine 18']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define feature engineering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def fill_nan(df):\n",
    "    \n",
    "    df.fillna('N', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def group_activities(df):\n",
    "    \n",
    "    df['Activity'] = np.where(df['Activity'].isin(GrindingRework), 'GrindingRework', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(Milling), 'Milling', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(FlatGrinding), 'FlatGrinding', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(RoundGrinding), 'RoundGrinding', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(TurningMilling), 'TurningMilling', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(SetupTurnMill), 'SetupTurnMill', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(Turning), 'Turning', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(TurnMillScrew), 'TurnMillScrew', df['Activity'])\n",
    "    df['Activity'] = np.where(df['Activity'].isin(WireCut), 'WireCut', df['Activity'])\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def group_resources(df):\n",
    "    \n",
    "    df['Resource'] = np.where(df['Resource'].isin(Grinding), 'Grinding', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(Milling), 'Milling', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(RoundGrinding), 'RoundGrinding', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(Turning_and_Milling), 'Turning_and_Milling', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(Turning), 'Turning', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(Quality_Check), 'Quality_Check', df['Resource'])\n",
    "    df['Resource'] = np.where(df['Resource'].isin(Wire_Cut), 'Wire_Cut', df['Resource'])\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_weekday_time(df):\n",
    "    \n",
    "    df['Start Timestamp'] =  pd.to_datetime(df['Start Timestamp'])\n",
    "    df['Complete Timestamp'] =  pd.to_datetime(df['Complete Timestamp'])\n",
    "    df['Start_Weekday'] = df['Start Timestamp'].dt.dayofweek\n",
    "    df['Start_Time[h]'] = df['Start Timestamp'].dt.hour\n",
    "    df['Start_Time[m]'] = df['Start Timestamp'].dt.minute\n",
    "    df['Start_Time[s]'] = df['Start_Time[h]'] * 3600 + df['Start_Time[m]'] * 60\n",
    "    df.drop(['Start_Time[h]', 'Start_Time[m]'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_avg_rem_leadtime(df):\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop_duplicates(subset='Case ID', keep='first', inplace=True)\n",
    "    df_copy = df_copy[['Part Desc.', 'RemainingTime[s]']]\n",
    "    df_copy = df_copy.groupby('Part Desc.').median().reset_index()\n",
    "    df_copy.rename(columns={'RemainingTime[s]': 'Avg_LeadTime[s]'}, inplace=True)\n",
    "    df = df.merge(df_copy, left_on='Part Desc.', right_on = 'Part Desc.', how='left')\n",
    "    \n",
    "    df['Avg_Rem_LeadTime[s]'] = df['Avg_LeadTime[s]'] - df['ElapsedTime[s]']\n",
    "    df.drop('Avg_LeadTime[s]', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "    \n",
    "def group_and_get_qty_finished(df):\n",
    "    \n",
    "    df['qty_fin'] = df.groupby(['Case ID', 'Activity'])['Qty Completed'].transform(pd.Series.cumsum)\n",
    "    df.drop_duplicates(subset=['Case ID', 'Activity'], keep='last', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_rem_act(df):\n",
    "    \n",
    "    df_copy = df.groupby('Case ID').size().reset_index(name='count')\n",
    "    df = df.merge(df_copy, left_on=['Case ID'], right_on = ['Case ID'], how='left')\n",
    "    df['sum_up'] = 1\n",
    "    df['n_kum'] = df.groupby('Case ID')['sum_up'].transform(pd.Series.cumsum)\n",
    "    df['rem_act'] = df['count'] - df['n_kum']\n",
    "    df['%_finished'] = df['n_kum'] / df['count']\n",
    "    df.drop(['count', 'sum_up', 'n_kum'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_finished(df):\n",
    "    \n",
    "    df['finished'] = 0\n",
    "    df['finished'] = np.where(df['RemainingTime[s]'] == 0.0, 1, df['finished'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def label_encode(df, cols):\n",
    "    \n",
    "    df[cols] = df[cols].apply(preprocessing.LabelEncoder().fit_transform)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_prefixes(df):\n",
    "    \n",
    "    df['Resource_Prefix'] = [y.Resource.tolist()[:z+1] for x, y in df.groupby('Case ID')for z in range(len(y))]\n",
    "    df['Activity_Prefix'] = [y.Activity.tolist()[:z+1] for x, y in df.groupby('Case ID')for z in range(len(y))]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_span(df):\n",
    "    \n",
    "    df['Span'] = df.apply(lambda x: x['Span'][1:], axis = 1)\n",
    "    df['Span'] = [x + ':00' for x in df['Span']]\n",
    "    df['Span'] = pd.to_timedelta(df['Span'])\n",
    "    df['Span'] = df['Span'].dt.total_seconds()\n",
    "    df.rename(columns={'Span': 'Span[s]'}, inplace=True)\n",
    "      \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = fill_nan(df)\n",
    "df3 = group_activities(df2)\n",
    "df4 = group_resources(df3)\n",
    "df5 = get_weekday_time(df4)\n",
    "df6 = get_avg_rem_leadtime(df5)\n",
    "df7 = group_and_get_qty_finished(df6)\n",
    "df8 = get_rem_act(df7)\n",
    "df9 = get_finished(df8)\n",
    "df10 = label_encode(df9, ['Activity', 'Resource', 'Part Desc.'])\n",
    "df11 = get_prefixes(df10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rearrange_order(df, alg):\n",
    "    \n",
    "    if alg == 'ANN':\n",
    "        df = df[['RemainingTime[s]', 'rem_act', '%_finished', 'ElapsedTime[s]', 'Avg_Rem_LeadTime[s]', \n",
    "                 'finished', 'Case ID']]\n",
    "        \n",
    "    elif alg == 'XGB':\n",
    "        df = df[['rem_DLZ_[m]', 'elapsed_time_compl[m]', 'rem_meth', 'rem_act', '%_finished', \n",
    "                 'rem_DLZ_[m]_avg', 'Spl_ID']]\n",
    "\n",
    "\n",
    "def set_index(df, idx_col):\n",
    "    \n",
    "    df.set_index(idx_col, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sw_cols(df):\n",
    "    \n",
    "    cols = df.columns\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "    not_sw_cols = set(cols) - set(num_cols)\n",
    "    sw_cols = set(cols) - set(not_sw_cols)\n",
    "    \n",
    "    return sw_cols \n",
    "\n",
    "\n",
    "def sliding_window(df, cols, w):\n",
    "    \n",
    "    for i in range(1, w+1):  \n",
    "        for ele in cols:\n",
    "            df[ele + '(t-' + str(i) + ')'] = df.groupby('Case ID')[ele].shift(i)\n",
    "    \n",
    "    df.fillna(0, inplace=True)\n",
    "    df.drop('Case ID', axis=1, inplace=True)\n",
    "\n",
    "    df.drop('RemainingTime[s](t-1)', axis=1, inplace=True)\n",
    "    df.drop('RemainingTime[s](t-2)', axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def split_data(df, train_size, alg):\n",
    "    \n",
    "    if alg == 'NN':\n",
    "        train = df.iloc[:train_size]\n",
    "        test = df.iloc[train_size:]\n",
    "        train_vals = train.values\n",
    "        test_vals = test.values\n",
    "        return train_vals, test_vals \n",
    "    \n",
    "    if alg == 'XGBoost':\n",
    "        train = df.iloc[:train_size]\n",
    "        test = df.iloc[train_size:]\n",
    "        return train, test\n",
    "    \n",
    "\n",
    "def scale(train, test, srange):\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=srange)\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.fit_transform(test)\n",
    "    \n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "\n",
    "def reshape_input(train, test, alg):\n",
    "    \n",
    "    if alg == 'ANN':\n",
    "        X, y = train[:, 1:], train[:, 0]\n",
    "        X_test, y_test = test[:, 1:], test[:, 0]\n",
    "        return X, y, X_test, y_test\n",
    "    \n",
    "    elif alg == 'XGBoost':\n",
    "        X, y = train.iloc[:, 1:], train.iloc[:, 0]\n",
    "        X_test, y_test = test.iloc[:, 1:], test.iloc[:, 0]\n",
    "        return X, y, X_test, y_test\n",
    "    \n",
    "    elif alg == 'LSTM':\n",
    "        X, y = train_scaled[:, 1:], train_scaled[:, 0]\n",
    "        X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "        X_test, y_test = test_scaled[:, 1:], test_scaled[:, 0]\n",
    "        X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "        return X, y, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Preprocessing for ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Create input matrix for numerical and label-encoded values\n",
    "df_small_ann = df11.drop(['Start Timestamp', 'Work Order  Qty', 'Worker ID', 'Report Type', \n",
    "                        'Qty Completed', 'Qty Rejected', 'Qty for MRB', 'Rework', 'Resource_Prefix', \n",
    "                        'Activity_Prefix', 'Span', 'Complete Timestamp', 'qty_fin', \n",
    "                          'Start_Weekday', 'Activity', 'Resource', 'Part Desc.', 'Start_Time[s]'], axis = 1)\n",
    "sw_cols = get_sw_cols(df_small_ann)\n",
    "df_small_ann = sliding_window(df_small_ann, sw_cols, 2)                                    \n",
    "train_vals, test_vals = split_data(df_small_ann, 1092, 'NN')\n",
    "ann_scaler, train_scaled, test_scaled = scale(train_vals, test_vals, (0, 1))\n",
    "X, y, X_test, y_test = reshape_input(train_scaled, test_scaled, 'ANN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions for Model building, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from numpy import concatenate\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "# =========================== Build Model ===========================\n",
    "\n",
    "def build_ann_model(model_type, neurons):\n",
    "    \n",
    "    if model_type == 'one_hidden_layer':    \n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(17, activation='relu', input_shape=[X.shape[1]]),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])       \n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'two_hidden_layers':\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(17, activation='relu', input_shape=[X.shape[1]]),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])         \n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'three_hidden_layers':\n",
    "        model = keras.models.Sequential([            \n",
    "            keras.layers.Dense(17, activation='relu', input_shape=[X.shape[1]]),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse']) \n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'four_hidden_layers':\n",
    "        model = keras.models.Sequential([            \n",
    "            keras.layers.Dense(17, activation='relu', input_shape=[X.shape[1]]),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(neurons, activation='relu'),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse']) \n",
    "        return model\n",
    "\n",
    "\n",
    "# =========================== Train Model ===========================\n",
    "\n",
    "def fit_ann_model(model, X, y, X_test, y_test):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "    checkpoint_cb = ModelCheckpoint('ann_model.h5', save_best_only=True)\n",
    "    \n",
    "    history = model.fit(X, y, batch_size=64, epochs=150, validation_data=(X_test, y_test),\n",
    "                            verbose=0, callbacks=[early_stopping, checkpoint_cb], shuffle=False)\n",
    "\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "# =========================== Test Model ===========================\n",
    "\n",
    "def test_ann_model(model, X_test, y_test, scaler):\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    X_orig = X_test.copy()\n",
    "    \n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, X_orig), axis=1)\n",
    "    inv_yhat = ann_scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "    # invert scaling for actual\n",
    "    y_test_orig = y_test.copy()\n",
    "    y_test_orig = y_test_orig.reshape(len(y_test_orig), 1)\n",
    "    inv_y = concatenate((y_test_orig, X_orig), axis=1)\n",
    "    inv_y = ann_scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "\n",
    "    # calculate errors\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))/3600\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)/3600\n",
    "    \n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model_1 = build_ann_model('one_hidden_layer', 500)\n",
    "ann_model_2 = build_ann_model('one_hidden_layer', 1000)\n",
    "ann_model_3 = build_ann_model('one_hidden_layer', 1500)\n",
    "ann_model_4 = build_ann_model('two_hidden_layers', 500)\n",
    "ann_model_5 = build_ann_model('two_hidden_layers', 1000)\n",
    "ann_model_6 = build_ann_model('two_hidden_layers', 1500)\n",
    "ann_model_7 = build_ann_model('three_hidden_layers', 500)\n",
    "ann_model_8 = build_ann_model('three_hidden_layers', 1000)\n",
    "ann_model_9 = build_ann_model('three_hidden_layers', 1500)\n",
    "ann_model_10 = build_ann_model('four_hidden_layers', 500)\n",
    "ann_model_11 = build_ann_model('four_hidden_layers', 1000)\n",
    "ann_model_12 = build_ann_model('four_hidden_layers', 1500)\n",
    "\n",
    "ann_model_list = [ann_model_1, ann_model_2, ann_model_3, ann_model_4, ann_model_5, ann_model_6,\n",
    "              ann_model_7, ann_model_8, ann_model_9, ann_model_10, ann_model_11, ann_model_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ann_model_list:\n",
    "        \n",
    "        model = fit_ann_model(model, X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ann_model_list:\n",
    "        \n",
    "        test_ann_model(model, X_test, y_test, ann_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Preprocessing for LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create input matrix for numerical and label-encoded values\n",
    "df_small_lstm = df11.drop(['Start Timestamp', 'Work Order  Qty', 'Worker ID', 'Report Type', \n",
    "                        'Qty Completed', 'Qty Rejected', 'Qty for MRB', 'Rework', 'Resource_Prefix', \n",
    "                        'Activity_Prefix', 'Span', 'qty_fin', 'Start_Weekday'], axis = 1)\n",
    "sw_cols = get_sw_cols(df_small_lstm)\n",
    "df_small_lstm = sliding_window(df_small_lstm, sw_cols, 2)\n",
    "df_small_lstm = set_index(df_small_lstm, 'Complete Timestamp')                                     \n",
    "train_vals, test_vals = split_data(df_small_lstm, 1092, 'NN')\n",
    "lstm_scaler, train_scaled, test_scaled = scale(train_vals, test_vals, (-1, 1))\n",
    "X, y, X_test, y_test = reshape_input(train_scaled, test_scaled, 'LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Functions for Model building, training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================== Build Model ===========================\n",
    "\n",
    "\n",
    "def build_lstm_model(model_type, neurons):\n",
    "    \n",
    "    if model_type == 'one_hidden_layer':\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.LSTM(neurons, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
    "            keras.layers.LSTM(neurons),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])\n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'two_hidden_layers':\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.LSTM(neurons, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])\n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'three_hidden_layers':\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.LSTM(neurons, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])\n",
    "        return model\n",
    "    \n",
    "    elif model_type == 'four_hidden_layers':\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.LSTM(neurons, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons, return_sequences=True),\n",
    "            keras.layers.LSTM(neurons),\n",
    "            keras.layers.Dense(1)])\n",
    "        \n",
    "        model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.optimizers.Adam(learning_rate=0.0001),\n",
    "                metrics=['mae', 'mse'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "# =========================== Train Model ===========================\n",
    "\n",
    "def fit_lstm_model(model, X, y, X_test, y_test):\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "    checkpoint_cb = ModelCheckpoint('lstm_model.h5', save_best_only=True)\n",
    "    \n",
    "    history = model.fit(X, y, batch_size=32, epochs=150, validation_data=(X_test, y_test), \n",
    "                        verbose=0, callbacks=[early_stopping, checkpoint_cb], shuffle=False)\n",
    "\n",
    "    # plot history\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================== Test Model ===========================\n",
    "\n",
    "def test_lstm_model(model, X_test, y_test, scaler):\n",
    "    \n",
    "    yhat = model.predict(X_test)\n",
    "    X_orig = X_test.copy()\n",
    "    X_orig = X_orig.reshape((X_orig.shape[0], X_orig.shape[2]))\n",
    "    \n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = concatenate((yhat, X_orig), axis=1)\n",
    "    inv_yhat = lstm_scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "    # invert scaling for actual\n",
    "    y_test_orig = y_test.copy()\n",
    "    y_test_orig = y_test_orig.reshape((len(y_test_orig), 1))\n",
    "    inv_y = concatenate((y_test_orig, X_orig), axis=1)\n",
    "    inv_y = lstm_scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "\n",
    "    # calculate errors\n",
    "    rmse = sqrt(mean_squared_error(inv_y, inv_yhat))/3600\n",
    "    mae = mean_absolute_error(inv_y, inv_yhat)/3600\n",
    "    \n",
    "    print('Test RMSE: %.3f' % rmse)\n",
    "    print('Test MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model_1 = build_lstm_model('one_hidden_layer', 100)\n",
    "lstm_model_2 = build_lstm_model('one_hidden_layer', 200)\n",
    "lstm_model_3 = build_lstm_model('one_hidden_layer', 300)\n",
    "lstm_model_4 = build_lstm_model('two_hidden_layers', 100)\n",
    "lstm_model_5 = build_lstm_model('two_hidden_layers', 200)\n",
    "lstm_model_6 = build_lstm_model('two_hidden_layers', 300)\n",
    "lstm_model_7 = build_lstm_model('three_hidden_layers', 100)\n",
    "lstm_model_8 = build_lstm_model('three_hidden_layers', 200)\n",
    "lstm_model_9 = build_lstm_model('three_hidden_layers', 300)\n",
    "lstm_model_10 = build_lstm_model('four_hidden_layers', 100)\n",
    "lstm_model_11 = build_lstm_model('four_hidden_layers', 200)\n",
    "lstm_model_12 = build_lstm_model('four_hidden_layers', 300)\n",
    "\n",
    "lstm_model_list = [lstm_model_1, lstm_model_2, lstm_model_3, lstm_model_4, lstm_model_5, lstm_model_6,\n",
    "                   lstm_model_7, lstm_model_8, lstm_model_9, lstm_model_10, lstm_model_11, lstm_model_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in lstm_model_list:\n",
    "        \n",
    "        model = fit_lstm_model(model, X, y, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in lstm_model_list:\n",
    "        \n",
    "        test_lstm_model(model, X_test, y_test, lstm_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Preprocessing for XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import re\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "\n",
    "#Create input matrix for numerical and label-encoded values\n",
    "df_small_xgb = df11.drop(['Start Timestamp', 'Work Order  Qty', 'Worker ID', 'Report Type', \n",
    "                        'Qty Completed', 'Qty Rejected', 'Qty for MRB', 'Rework', 'Resource_Prefix', \n",
    "                        'Activity_Prefix', 'Span', 'Complete Timestamp'], axis = 1)\n",
    "sw_cols = get_sw_cols(df_small_xgb)\n",
    "df_small_xgb = sliding_window(df_small_xgb, sw_cols, 2) \n",
    "df_small_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<')))\n",
    "                         else col for col in df_small_xgb.columns.values]\n",
    "df_small_xgb.rename(columns = lambda x: x.replace(' ', '_'), inplace=True)\n",
    "train, test = split_data(df_small_xgb, 1092, 'XGBoost')\n",
    "X, y, X_test, y_test = reshape_input(train, test, 'XGBoost')\n",
    "train_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Function for Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error'):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    pred = fitted_model.predict(X_test_data)\n",
    "    \n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBRegressor()\n",
    "param_grid = {\n",
    "    'objective': ['reg:linear', 'reg:logistic'],\n",
    "    'colsample_bytree': [0.3, 0.4, 0.5],\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'alpha': [5, 10, 20],\n",
    "    'learning_rate': [0.1, 0.01, 0.05]}\n",
    "\n",
    "model, pred = algorithm_pipeline(X, X_test, y, y_test, model, \n",
    "                                 param_grid, cv=2)\n",
    "\n",
    "print(np.sqrt(-model.best_score_)/3600)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                          max_depth = 5, alpha = 10, n_estimators = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_xgb = xg_reg.predict(X_test)\n",
    "\n",
    "# calculate errors\n",
    "rmse_xgb = sqrt(mean_squared_error(y_test, yhat_xgb))/3600\n",
    "mae_xgb = mean_absolute_error(y_test, yhat_xgb)/3600\n",
    "    \n",
    "print('Deep Model Test RMSE: %.3f' % rmse_xgb)\n",
    "print('Deep Model Test MAE: %.3f' % mae_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
